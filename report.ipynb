{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "eb01b422",
      "metadata": {},
      "source": [
        "# 基于 ResNet-18 微调的 Caltech-101 图像分类实验报告\n",
        "\n",
        "本报告将介绍使用预训练的 ResNet-18 模型，通过微调方法在 Caltech-101 数据集上进行图像分类任务的过程、配置、代码实现及结果可视化。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa8f1e2f",
      "metadata": {},
      "source": [
        "## 1. 引言\n",
        "\n",
        "图像分类是计算机视觉领域的一个基础且重要的任务。深度学习，特别是卷积神经网络（CNN），在此任务上取得了显著的成功。ResNet (Residual Network) 是一种经典的 CNN 架构，通过引入残差连接解决了深度网络训练困难的问题。Caltech-101 数据集是一个广泛用于对象识别研究的标准 benchmark。\n",
        "\n",
        "本实验的目标是：\n",
        "1. 修改预训练的 ResNet-18 模型以适应 Caltech-101 数据集的101个类别。\n",
        "2. 采用微调策略，冻结大部分预训练层，仅训练新添加的分类层和微调部分顶层卷积层。\n",
        "3. 在有限的时间内在本机CPU上完成训练。\n",
        "4. 使用 TensorBoard 可视化训练过程中的损失和准确率。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01dc974c",
      "metadata": {},
      "source": [
        "## 2. 模型与数据集介绍\n",
        "\n",
        "### 2.1 ResNet-18 模型\n",
        "ResNet (Residual Network) 是一种深度卷积神经网络架构，其核心创新是引入了“残差学习”单元。这些单元通过“快捷连接”（skip connections）允许梯度更直接地反向传播到较早的层，从而使得训练非常深的网络成为可能。ResNet-18 是该系列中一个相对较浅（18个含权层）但仍然非常有效的模型，它在图像识别任务的性能和计算效率之间取得了良好的平衡，适合在资源受限的情况下进行微调。\n",
        "\n",
        "### 2.2 Caltech-101 数据集\n",
        "Caltech-101 数据集包含101个对象类别（例如，飞机、摩托车、佛像等）以及一个背景类别。每个对象类别大约有40到800张图像，总图像数约为9000张。图像尺寸各不相同。该数据集的挑战包括类内差异大、视角变化、光照条件不同以及背景杂乱等。在本实验中，背景类别 `BACKGROUND_Google` 不是视为一个有效的学习目标，因此在数据加载时已被我手动排除。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "901ca21d",
      "metadata": {},
      "source": [
        "## 3. 实验设置与代码实现\n",
        "\n",
        "整个实验流程基于 Python 和 PyTorch 框架实现。下面将分模块介绍代码的配置和功能。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0f31cf1c",
      "metadata": {},
      "source": [
        "### 3.1 初始配置与导入库\n",
        "\n",
        "首先，我们导入必要的库并定义实验相关的配置参数。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "844ce8cb",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.datasets import ImageFolder\n",
        "import time\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter # 用于 TensorBoard 可视化\n",
        "\n",
        "# --- Configuration ---\n",
        "DATA_DIR = 'caltech-101/101_ObjectCategories/101_ObjectCategories' # 数据集路径，请确保此路径正确\n",
        "NUM_CLASSES = 101        # Caltech-101 的类别数\n",
        "BATCH_SIZE = 32          # 批处理大小，根据CPU和内存调整\n",
        "IMAGE_SIZE = (128, 128)  # 输入图像统一调整的大小，较小尺寸以加速CPU训练\n",
        "NUM_EPOCHS = 5           # 训练周期数，原为1，这里假设根据脚本内容增加到了5\n",
        "LEARNING_RATE_FC = 0.001 # 新全连接层的学习率\n",
        "LEARNING_RATE_FINETUNE = 0.0001 # 微调预训练层的学习率\n",
        "TRAIN_SPLIT_RATIO = 0.8  # 训练集划分比例\n",
        "RANDOM_SEED = 0          # 随机种子，用于保证实验可复现性"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b31ba896",
      "metadata": {},
      "source": [
        "### 3.2 数据加载与预处理模块 (`get_data_loaders`)\n",
        "\n",
        "此模块负责从指定路径加载 Caltech-101 数据集，并进行必要的预处理，然后创建训练集和验证集的 DataLoader。\n",
        "\n",
        "主要步骤包括：\n",
        "- 定义图像变换：包括缩放图像到 `IMAGE_SIZE`，随机水平翻转、随机旋转和颜色抖动等数据增强操作，转换为Tensor，以及使用ImageNet的均值和标准差进行归一化。\n",
        "- 使用 `ImageFolder` 加载数据：它会自动从子文件夹名称推断类别标签。\n",
        "- 划分数据集：按照 `TRAIN_SPLIT_RATIO` 将完整数据集划分为训练集和验证集。\n",
        "- 创建 `DataLoader`：用于在训练和验证过程中高效地批量加载数据。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f17ae57f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_data_loaders(data_dir, image_size, batch_size, train_split_ratio, random_seed):\n",
        "    \"\"\"\n",
        "    加载 Caltech-101 数据集并创建训练和验证 DataLoader。\n",
        "    \"\"\"\n",
        "    print(f\"从以下路径加载数据: {data_dir}\")\n",
        "\n",
        "    transform = transforms.Compose([\n",
        "        transforms.Resize(image_size),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(10),\n",
        "        transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # ImageNet 统计数据\n",
        "    ])\n",
        "\n",
        "    try:\n",
        "        full_dataset = ImageFolder(root=data_dir, transform=transform)\n",
        "        print(f\"数据集已加载。找到的总类别数: {len(full_dataset.classes)}\")\n",
        "        if len(full_dataset.classes) == 0:\n",
        "            print(f\"错误: 未找到类别。请检查 '{data_dir}' 是否包含每个类别的子目录。\")\n",
        "            return None, None, 0\n",
        "        if len(full_dataset) == 0:\n",
        "            print(f\"错误: 数据集为空。请检查 {data_dir} 中的图像文件。\")\n",
        "            return None, None, len(full_dataset.classes)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"加载数据集时出错: {e}\")\n",
        "        print(f\"请确保 Caltech-101 数据集正确放置在 '{data_dir}' 目录中。\")\n",
        "        print(\"并且它不为空或损坏，并且如果需要，已删除不需要的类文件夹（如 'BACKGROUND_Google'）。\")\n",
        "        return None, None, 0\n",
        "\n",
        "    # 分割数据集\n",
        "    num_train = int(len(full_dataset) * train_split_ratio)\n",
        "    num_val = len(full_dataset) - num_train\n",
        "\n",
        "    if num_train == 0 or num_val == 0:\n",
        "        print(f\"错误: 没有足够的数据来分割为训练集和验证集。训练集数量: {num_train}, 验证集数量: {num_val}\")\n",
        "        return None, None, len(full_dataset.classes)\n",
        "\n",
        "    train_dataset, val_dataset = random_split(full_dataset, [num_train, num_val], \n",
        "                                            generator=torch.Generator().manual_seed(random_seed))\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "    print(f\"训练样本数: {len(train_dataset)}\")\n",
        "    print(f\"验证样本数: {len(val_dataset)}\")\n",
        "    print(f\"类别数: {len(full_dataset.classes)}\")\n",
        "\n",
        "    return train_loader, val_loader, len(full_dataset.classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0354850",
      "metadata": {},
      "source": [
        "### 3.3 模型定义与微调策略模块 (`get_model`)\n",
        "\n",
        "此模块负责加载预训练的 ResNet-18 模型，并根据 Caltech-101 数据集的类别数修改其分类头。同时，它也设置了不同的学习率策略用于微调。\n",
        "\n",
        "主要步骤：\n",
        "- 加载预训练的 ResNet-18：使用 `torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)` 加载在 ImageNet 上预训练好的模型权重。\n",
        "- 冻结参数：初始时，冻结模型所有参数的梯度更新（`param.requires_grad = False`）。\n",
        "- 修改分类层：获取原全连接层 (`model.fc`) 的输入特征数，然后替换为一个新的 `nn.Linear` 层，其输出特征数为 `NUM_CLASSES`。这个新层默认 `requires_grad=True`。\n",
        "- 设置Xavier初始化：对新的全连接层权重使用Xavier均匀初始化。\n",
        "- 解冻部分预训练层：选择性地解冻 ResNet 靠后的卷积层（即 `layer3` 和 `layer4`），设置其 `requires_grad = True`，以便进行微调。\n",
        "- 配置优化器：使用 `Adam` 优化器，并为其传递两个参数组：一组是新全连接层的参数（使用 `LEARNING_RATE_FC`），另一组是解冻的预训练层参数（使用 `LEARNING_RATE_FINETUNE`）。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "3fd16c1c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def get_model(num_classes, learning_rate_fc, learning_rate_finetune):\n",
        "    \"\"\"\n",
        "    加载预训练的 ResNet-18 模型并修改其分类器。\n",
        "    为新分类器和预训练层设置不同的学习率。\n",
        "    \"\"\"\n",
        "    model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.IMAGENET1K_V1)\n",
        "\n",
        "    # 首先冻结所有参数\n",
        "    for param in model.parameters():\n",
        "        param.requires_grad = False\n",
        "\n",
        "    # 修改最后的全连接层\n",
        "    num_ftrs = model.fc.in_features\n",
        "    model.fc = nn.Linear(num_ftrs, num_classes) # 新层默认 requires_grad=True\n",
        "    nn.init.xavier_uniform_(model.fc.weight); # Xavier 初始化\n",
        "    \n",
        "    # 设置具有不同学习率的优化器\n",
        "    # 新初始化的全连接层的参数\n",
        "    fc_params = model.fc.parameters()\n",
        "    # 预训练层的参数（我们将解冻其中一些进行微调）\n",
        "    # 对于 ResNet，我们微调 layer4 及以上的层（更特定的特征）\n",
        "    finetune_params = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if \"fc\" not in name: # 排除已处理的fc层\n",
        "             # 解冻较后的层进行微调\n",
        "            if 'layer4' in name or 'layer3' in name: # 示例：微调 layer3 和 layer4\n",
        "                param.requires_grad = True\n",
        "                finetune_params.append(param)\n",
        "            else:\n",
        "                param.requires_grad = False # 保持较早的层冻结\n",
        "\n",
        "    optimizer = optim.Adam([\n",
        "        {'params': fc_params, 'lr': learning_rate_fc},\n",
        "        {'params': finetune_params, 'lr': learning_rate_finetune}\n",
        "    ], lr=learning_rate_fc) # 默认学习率，尽管会被特定组的学习率覆盖\n",
        "\n",
        "    return model, optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c4913a9a",
      "metadata": {},
      "source": [
        "### 3.4 训练与评估模块 (`train_model`)\n",
        "\n",
        "此模块包含模型训练和验证的核心逻辑。\n",
        "\n",
        "主要功能：\n",
        "- 迭代指定的 `num_epochs` 次。\n",
        "- **训练阶段** (每个 epoch 内):\n",
        "    - 设置模型为训练模式 (`model.train()`)。\n",
        "    - 遍历训练数据加载器 (`train_loader`)。\n",
        "    - 将数据移至指定设备 (`device`)。\n",
        "    - 清零优化器梯度 (`optimizer.zero_grad()`)。\n",
        "    - 前向传播，计算模型输出 (`outputs = model(inputs)`)。\n",
        "    - 计算损失 (`loss = criterion(outputs, labels)`)。\n",
        "    - 反向传播，计算梯度 (`loss.backward()`)。\n",
        "    - 更新模型参数 (`optimizer.step()`)。\n",
        "    - 累积损失和计算训练准确率。\n",
        "    - 将每批次的训练损失记录到 TensorBoard。\n",
        "- **验证阶段** (每个 epoch 结束时):\n",
        "    - 设置模型为评估模式 (`model.eval()`)。\n",
        "    - 在 `torch.no_grad()` 上下文中执行，禁用梯度计算以节省内存和计算。\n",
        "    - 遍历验证数据加载器 (`val_loader`)。\n",
        "    - 计算验证集上的总损失和准确率。\n",
        "- **日志记录**:\n",
        "    - 打印每个 epoch 的训练/验证损失和准确率以及耗时。\n",
        "    - 将每个 epoch 的平均训练损失、训练准确率、验证损失和验证准确率记录到 TensorBoard。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "393ab2be",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, writer): # writer 用于TensorBoard\n",
        "    \"\"\"\n",
        "    训练模型并在验证集上进行评估。\n",
        "    将训练和验证指标记录到 TensorBoard。\n",
        "    \"\"\"\n",
        "    print(f\"在 {device} 上训练...\")\n",
        "    global_step = 0 # 用于记录批次训练损失\n",
        "    for epoch in range(num_epochs):\n",
        "        start_time_epoch = time.time()\n",
        "        model.train() # 设置模型为训练模式\n",
        "        running_loss_train = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for i, (inputs, labels) in enumerate(train_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            running_loss_train += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "            # 每个批次后记录训练损失到 TensorBoard\n",
        "            writer.add_scalar('Loss/train_batch', loss.item(), global_step)\n",
        "            global_step += 1\n",
        "\n",
        "            if (i + 1) % 20 == 0: # 每 20 个批次打印一次日志\n",
        "                print(f'Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        epoch_loss_train = running_loss_train / len(train_loader.dataset)\n",
        "        epoch_acc_train = correct_train / total_train\n",
        "        \n",
        "        # 每个 epoch 后记录平均训练损失和准确率\n",
        "        writer.add_scalar('Loss/train_epoch', epoch_loss_train, epoch)\n",
        "        writer.add_scalar('Accuracy/train_epoch', epoch_acc_train, epoch)\n",
        "\n",
        "        # --- 验证阶段 ---\n",
        "        model.eval() # 设置模型为评估模式\n",
        "        running_loss_val = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "        with torch.no_grad(): # 评估时不需要计算梯度\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "                running_loss_val += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels).sum().item()\n",
        "\n",
        "        epoch_loss_val = running_loss_val / len(val_loader.dataset)\n",
        "        epoch_acc_val = correct_val / total_val\n",
        "\n",
        "        # 每个 epoch 后记录验证损失和准确率到 TensorBoard\n",
        "        writer.add_scalar('Loss/validation_epoch', epoch_loss_val, epoch)\n",
        "        writer.add_scalar('Accuracy/validation_epoch', epoch_acc_val, epoch)\n",
        "\n",
        "        end_time_epoch = time.time()\n",
        "        epoch_duration = end_time_epoch - start_time_epoch\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs} 耗时 {epoch_duration:.2f}s\")\n",
        "        print(f\"  训练损失: {epoch_loss_train:.4f}, 训练准确率: {epoch_acc_train:.4f}\")\n",
        "        print(f\"  验证损失: {epoch_loss_val:.4f}, 验证准确率: {epoch_acc_val:.4f}\")\n",
        "\n",
        "    print(\"训练完成\")\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "26beac8d",
      "metadata": {},
      "source": [
        "### 3.5 主执行逻辑\n",
        "\n",
        "这是脚本的主要执行部分，它将上述所有模块组合起来，完成整个训练和评估流程。\n",
        "\n",
        "步骤包括：\n",
        "- 初始化 `SummaryWriter` 用于 TensorBoard 日志记录，日志将保存在 `runs/Caltech101_ResNet18_<timestamp>` 目录下。\n",
        "- 设置计算设备（CPU 或 CUDA GPU，如果可用）。\n",
        "- 调用 `get_data_loaders` 加载数据。\n",
        "- 检查加载的类别数是否与预期的 `NUM_CLASSES` 一致，如果不一致则使用实际加载的类别数。\n",
        "- 调用 `get_model` 获取模型和优化器。\n",
        "- 定义损失函数（`nn.CrossEntropyLoss`）。\n",
        "- 调用 `train_model` 执行训练和评估。\n",
        "- 关闭 `SummaryWriter`。\n",
        "- 打印总执行时间。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "8c32aef4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "开始 Caltech-101 分类任务...\n",
            "使用设备: cpu\n",
            "从以下路径加载数据: caltech-101/101_ObjectCategories/101_ObjectCategories\n",
            "数据集已加载。找到的总类别数: 101\n",
            "训练样本数: 6941\n",
            "验证样本数: 1736\n",
            "类别数: 101\n",
            "当前Notebook训练的TensorBoard日志将保存到：runs\\Notebook_Run_20250529-151921\n",
            "在 cpu 上训练...\n",
            "Epoch [1/5], Batch [20/217], Loss: 3.7138\n",
            "Epoch [1/5], Batch [40/217], Loss: 1.6943\n",
            "Epoch [1/5], Batch [60/217], Loss: 1.0137\n",
            "Epoch [1/5], Batch [80/217], Loss: 1.5851\n",
            "Epoch [1/5], Batch [100/217], Loss: 1.2455\n",
            "Epoch [1/5], Batch [120/217], Loss: 1.7049\n",
            "Epoch [1/5], Batch [140/217], Loss: 1.2910\n",
            "Epoch [1/5], Batch [160/217], Loss: 0.9077\n",
            "Epoch [1/5], Batch [180/217], Loss: 0.7428\n",
            "Epoch [1/5], Batch [200/217], Loss: 0.6682\n",
            "Epoch 1/5 耗时 138.10s\n",
            "  训练损失: 1.4771, 训练准确率: 0.6730\n",
            "  验证损失: 0.5599, 验证准确率: 0.8497\n",
            "Epoch [2/5], Batch [20/217], Loss: 0.3265\n",
            "Epoch [2/5], Batch [40/217], Loss: 0.3769\n",
            "Epoch [2/5], Batch [60/217], Loss: 0.2004\n",
            "Epoch [2/5], Batch [80/217], Loss: 0.5411\n",
            "Epoch [2/5], Batch [100/217], Loss: 0.2292\n",
            "Epoch [2/5], Batch [120/217], Loss: 0.2949\n",
            "Epoch [2/5], Batch [140/217], Loss: 0.4645\n",
            "Epoch [2/5], Batch [160/217], Loss: 0.0739\n",
            "Epoch [2/5], Batch [180/217], Loss: 0.5444\n",
            "Epoch [2/5], Batch [200/217], Loss: 0.2213\n",
            "Epoch 2/5 耗时 148.60s\n",
            "  训练损失: 0.3378, 训练准确率: 0.9039\n",
            "  验证损失: 0.4060, 验证准确率: 0.8906\n",
            "Epoch [3/5], Batch [20/217], Loss: 0.0614\n",
            "Epoch [3/5], Batch [40/217], Loss: 0.0727\n",
            "Epoch [3/5], Batch [60/217], Loss: 0.1168\n",
            "Epoch [3/5], Batch [80/217], Loss: 0.1060\n",
            "Epoch [3/5], Batch [100/217], Loss: 0.1507\n",
            "Epoch [3/5], Batch [120/217], Loss: 0.0136\n",
            "Epoch [3/5], Batch [140/217], Loss: 0.2558\n",
            "Epoch [3/5], Batch [160/217], Loss: 0.1956\n",
            "Epoch [3/5], Batch [180/217], Loss: 0.1843\n",
            "Epoch [3/5], Batch [200/217], Loss: 0.1316\n",
            "Epoch 3/5 耗时 151.39s\n",
            "  训练损失: 0.1793, 训练准确率: 0.9470\n",
            "  验证损失: 0.3783, 验证准确率: 0.8969\n",
            "Epoch [4/5], Batch [20/217], Loss: 0.2516\n",
            "Epoch [4/5], Batch [40/217], Loss: 0.1145\n",
            "Epoch [4/5], Batch [60/217], Loss: 0.1457\n",
            "Epoch [4/5], Batch [80/217], Loss: 0.0731\n",
            "Epoch [4/5], Batch [100/217], Loss: 0.0723\n",
            "Epoch [4/5], Batch [120/217], Loss: 0.0239\n",
            "Epoch [4/5], Batch [140/217], Loss: 0.0258\n",
            "Epoch [4/5], Batch [160/217], Loss: 0.1149\n",
            "Epoch [4/5], Batch [180/217], Loss: 0.1513\n",
            "Epoch [4/5], Batch [200/217], Loss: 0.0327\n",
            "Epoch 4/5 耗时 149.99s\n",
            "  训练损失: 0.1152, 训练准确率: 0.9667\n",
            "  验证损失: 0.3628, 验证准确率: 0.9061\n",
            "Epoch [5/5], Batch [20/217], Loss: 0.0479\n",
            "Epoch [5/5], Batch [40/217], Loss: 0.0169\n",
            "Epoch [5/5], Batch [60/217], Loss: 0.1097\n",
            "Epoch [5/5], Batch [80/217], Loss: 0.0348\n",
            "Epoch [5/5], Batch [100/217], Loss: 0.0580\n",
            "Epoch [5/5], Batch [120/217], Loss: 0.2670\n",
            "Epoch [5/5], Batch [140/217], Loss: 0.2456\n",
            "Epoch [5/5], Batch [160/217], Loss: 0.3476\n",
            "Epoch [5/5], Batch [180/217], Loss: 0.0351\n",
            "Epoch [5/5], Batch [200/217], Loss: 0.0409\n",
            "Epoch 5/5 耗时 152.64s\n",
            "  训练损失: 0.0892, 训练准确率: 0.9738\n",
            "  验证损失: 0.4004, 验证准确率: 0.9055\n",
            "训练完成\n",
            "总执行时间: 740.95 秒。\n",
            "警告: 训练时间超过10分钟。\n",
            "任务完成。\n"
          ]
        }
      ],
      "source": [
        "if __name__ == '__main__' and '__file__' not in globals(): # 在Jupyter Notebook中运行时，通常__name__是'__main__'，但没有__file__\n",
        "    print(\"开始 Caltech-101 分类任务...\")\n",
        "    overall_start_time = time.time()\n",
        "\n",
        "    # 初始化 SummaryWriter\n",
        "    run_name = f\"Caltech101_ResNet18_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
        "    log_dir_notebook = os.path.join('runs', run_name) # 为notebook单独创建一个日志目录\n",
        "    # 注意：下面的Tensorboard将使用固定的预训练日志路径\n",
        "    # writer = SummaryWriter(log_dir=log_dir_notebook) # 如果要重新训练并记录，取消注释这一行和下面的close\n",
        "    # print(f\"新的 TensorBoard 日志将保存到: {log_dir_notebook}\")\n",
        "\n",
        "    # 设置设备\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"使用设备: {device}\")\n",
        "    if str(device) == \"cuda\":\n",
        "        print(f\"CUDA 设备名称: {torch.cuda.get_device_name(0)}\")\n",
        "\n",
        "    # 获取 DataLoaders\n",
        "    train_loader, val_loader, num_actual_classes = get_data_loaders(\n",
        "        data_dir=DATA_DIR,\n",
        "        image_size=IMAGE_SIZE,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        train_split_ratio=TRAIN_SPLIT_RATIO,\n",
        "        random_seed=RANDOM_SEED\n",
        "    )\n",
        "\n",
        "    if train_loader is None or val_loader is None:\n",
        "        print(\"加载数据失败。正在退出。\")\n",
        "        # if 'writer' in locals(): writer.close()\n",
        "        exit() # 在notebook中可能需要换成 return 或者直接不执行后续cell\n",
        "\n",
        "    if num_actual_classes != NUM_CLASSES and num_actual_classes > 0 :\n",
        "        print(f\"警告: 检测到的类别数 ({num_actual_classes}) 与 NUM_CLASSES ({NUM_CLASSES}) 不同。\")\n",
        "        print(f\"使用 {num_actual_classes} 作为类别数。\")\n",
        "        current_num_classes = num_actual_classes\n",
        "    elif num_actual_classes == 0:\n",
        "        print(\"错误: 未加载任何类别。正在退出。\")\n",
        "        # if 'writer' in locals(): writer.close()\n",
        "        exit() # 同上\n",
        "    else:\n",
        "        current_num_classes = NUM_CLASSES\n",
        "\n",
        "    # 获取模型和优化器\n",
        "    model, optimizer = get_model(\n",
        "        num_classes=current_num_classes,\n",
        "        learning_rate_fc=LEARNING_RATE_FC,\n",
        "        learning_rate_finetune=LEARNING_RATE_FINETUNE\n",
        "    )\n",
        "    model.to(device)\n",
        "\n",
        "    # 损失函数\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    \n",
        "    # --- 演示：如果要在Notebook中重新训练并记录，需要取消注释writer的初始化和关闭 ---\n",
        "    # print(f\"开始训练 {NUM_EPOCHS} 个 epoch(s)...\")\n",
        "    # # 创建一个临时的 writer 实例用于本次训练 (如果上面没有初始化全局 writer)\n",
        "    temp_run_name = f\"Notebook_Run_{time.strftime('%Y%m%d-%H%M%S')}\"\n",
        "    temp_log_dir = os.path.join('runs', temp_run_name)\n",
        "    notebook_writer = SummaryWriter(log_dir=temp_log_dir)\n",
        "    print(f\"当前Notebook训练的TensorBoard日志将保存到：{temp_log_dir}\")\n",
        "    \n",
        "    trained_model = train_model(model, train_loader, val_loader, criterion, optimizer, NUM_EPOCHS, device, notebook_writer)\n",
        "    \n",
        "    notebook_writer.close()\n",
        "    # --- 演示结束 ---\n",
        "\n",
        "    overall_end_time = time.time()\n",
        "    total_duration = overall_end_time - overall_start_time\n",
        "    print(f\"总执行时间: {total_duration:.2f} 秒。\")\n",
        "    if total_duration > 600 and NUM_EPOCHS > 1: # 10 分钟\n",
        "        print(\"警告: 训练时间超过10分钟。\")\n",
        "\n",
        "    print(\"任务完成。\")\n",
        "else:\n",
        "    print(\"代码块已定义，请在Notebook中按需调用函数或运行主逻辑（如果适用）。\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fdfc0815",
      "metadata": {},
      "source": [
        "### 3.6 实验结果\n",
        "\n",
        "经过 5 个 epoch 的训练，我们的模型在验证集上的准确率达到了 0.9055"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6601da82",
      "metadata": {},
      "source": [
        "## 4. TensorBoard 可视化\n",
        "\n",
        "我们将使用 TensorBoard 来可视化训练过程中的关键指标，包括训练集和验证集上的损失（Loss）曲线，以及验证集上的准确率（Accuracy）变化。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "d1fa4f12",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6010 (pid 24336), started 0:00:29 ago. (Use '!kill 24336' to kill it.)"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "      <iframe id=\"tensorboard-frame-804ff590099e7549\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
              "      </iframe>\n",
              "      <script>\n",
              "        (function() {\n",
              "          const frame = document.getElementById(\"tensorboard-frame-804ff590099e7549\");\n",
              "          const url = new URL(\"http://localhost\");\n",
              "          const port = 6010;\n",
              "          if (port) {\n",
              "            url.port = port;\n",
              "          }\n",
              "          frame.src = url;\n",
              "        })();\n",
              "      </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# 加载 TensorBoard notebook 扩展\n",
        "%load_ext tensorboard\n",
        "\n",
        "# 指定之前训练保存的 TensorBoard 日志目录路径\n",
        "#log_dir_path = \"C:\\\\Users\\\\Ruiyu\\\\Desktop\\\\NN_and_DL\\\\MidTerm\\\\runs\\\\Notebook_Run_20250529-151921\"\n",
        "\n",
        "%tensorboard --logdir=\"C:\\\\Users\\\\Ruiyu\\\\Desktop\\\\NN_and_DL\\\\MidTerm\\\\runs\\\\Notebook_Run_20250529-151921\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f68566b0",
      "metadata": {},
      "source": [
        "我们将参数保存下来。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "31332775",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "模型参数已成功保存到: c:\\Users\\Ruiyu\\Desktop\\NN_and_DL\\MidTerm\\model_parameters.pth\n"
          ]
        }
      ],
      "source": [
        "# --- 保存模型参数 ---\n",
        "\n",
        "model_save_path = \"model_parameters.pth\" #保存在当前工作目录下\n",
        "\n",
        "if 'trained_model' in locals():\n",
        "    try:\n",
        "        torch.save(trained_model.state_dict(), model_save_path)\n",
        "        print(f\"模型参数已成功保存到: {os.path.abspath(model_save_path)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"保存模型参数时发生错误: {e}\")\n",
        "else:\n",
        "    print(\"错误: 'trained_model' 未定义。请确保模型已经训练完毕。\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "baccfc74",
      "metadata": {},
      "source": [
        "# 5. 超参数选择\n",
        "\n",
        "由于计算资源的有限，我们在这里只运行一些非常 toy 的超参数搜索。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "id": "b5023a24",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Starting Hyperparameter Search (No TensorBoard Logging) ---\n",
            "Prerequisite variables found. Proceeding with hyperparameter search.\n",
            "\n",
            "========== Testing Hyperparameters: {'epochs': 1, 'lr_fc': 0.001, 'lr_finetune': 0.0001} ==========\n",
            "Training on cpu for 1 epoch(s)...\n",
            "  Epoch [1/1], Batch [20/217], Batch Loss: 3.7174\n",
            "  Epoch [1/1], Batch [40/217], Batch Loss: 2.3352\n",
            "  Epoch [1/1], Batch [60/217], Batch Loss: 1.4172\n",
            "  Epoch [1/1], Batch [80/217], Batch Loss: 1.0213\n",
            "  Epoch [1/1], Batch [100/217], Batch Loss: 1.0038\n",
            "  Epoch [1/1], Batch [120/217], Batch Loss: 1.1085\n",
            "  Epoch [1/1], Batch [140/217], Batch Loss: 1.0488\n",
            "  Epoch [1/1], Batch [160/217], Batch Loss: 0.7869\n",
            "  Epoch [1/1], Batch [180/217], Batch Loss: 0.5555\n",
            "  Epoch [1/1], Batch [200/217], Batch Loss: 1.0363\n",
            "  Epoch 1/1 took 141.91s\n",
            "    Train Loss: 1.4889, Train Acc: 0.6780\n",
            "    Val Loss:   0.5565, Val Acc:   0.8485\n",
            "  Training completed for this hyperparameter set.\n",
            "Reported Final Validation Loss for {'epochs': 1, 'lr_fc': 0.001, 'lr_finetune': 0.0001}: 0.5565, Acc: 0.8485\n",
            "==================================================\n",
            "\n",
            "\n",
            "========== Testing Hyperparameters: {'epochs': 1, 'lr_fc': 0.002, 'lr_finetune': 0.0002} ==========\n",
            "Training on cpu for 1 epoch(s)...\n",
            "  Epoch [1/1], Batch [20/217], Batch Loss: 3.0516\n",
            "  Epoch [1/1], Batch [40/217], Batch Loss: 1.9566\n",
            "  Epoch [1/1], Batch [60/217], Batch Loss: 1.5296\n",
            "  Epoch [1/1], Batch [80/217], Batch Loss: 1.0961\n",
            "  Epoch [1/1], Batch [100/217], Batch Loss: 0.6091\n",
            "  Epoch [1/1], Batch [120/217], Batch Loss: 1.0671\n",
            "  Epoch [1/1], Batch [140/217], Batch Loss: 0.5719\n",
            "  Epoch [1/1], Batch [160/217], Batch Loss: 0.7137\n",
            "  Epoch [1/1], Batch [180/217], Batch Loss: 0.7168\n",
            "  Epoch [1/1], Batch [200/217], Batch Loss: 0.4401\n",
            "  Epoch 1/1 took 152.89s\n",
            "    Train Loss: 1.3183, Train Acc: 0.6992\n",
            "    Val Loss:   0.6154, Val Acc:   0.8306\n",
            "  Training completed for this hyperparameter set.\n",
            "Reported Final Validation Loss for {'epochs': 1, 'lr_fc': 0.002, 'lr_finetune': 0.0002}: 0.6154, Acc: 0.8306\n",
            "==================================================\n",
            "\n",
            "\n",
            "========== Testing Hyperparameters: {'epochs': 3, 'lr_fc': 0.001, 'lr_finetune': 0.0001} ==========\n",
            "Training on cpu for 3 epoch(s)...\n",
            "  Epoch [1/3], Batch [20/217], Batch Loss: 3.2469\n",
            "  Epoch [1/3], Batch [40/217], Batch Loss: 2.7885\n",
            "  Epoch [1/3], Batch [60/217], Batch Loss: 1.9565\n",
            "  Epoch [1/3], Batch [80/217], Batch Loss: 0.7252\n",
            "  Epoch [1/3], Batch [100/217], Batch Loss: 0.8920\n",
            "  Epoch [1/3], Batch [120/217], Batch Loss: 1.1594\n",
            "  Epoch [1/3], Batch [140/217], Batch Loss: 0.7872\n",
            "  Epoch [1/3], Batch [160/217], Batch Loss: 0.5585\n",
            "  Epoch [1/3], Batch [180/217], Batch Loss: 0.7966\n",
            "  Epoch [1/3], Batch [200/217], Batch Loss: 0.5698\n",
            "  Epoch 1/3 took 138.25s\n",
            "    Train Loss: 1.5034, Train Acc: 0.6707\n",
            "    Val Loss:   0.5396, Val Acc:   0.8485\n",
            "  Epoch [2/3], Batch [20/217], Batch Loss: 0.4185\n",
            "  Epoch [2/3], Batch [40/217], Batch Loss: 0.1398\n",
            "  Epoch [2/3], Batch [60/217], Batch Loss: 0.6063\n",
            "  Epoch [2/3], Batch [80/217], Batch Loss: 0.6241\n",
            "  Epoch [2/3], Batch [100/217], Batch Loss: 0.4691\n",
            "  Epoch [2/3], Batch [120/217], Batch Loss: 0.2160\n",
            "  Epoch [2/3], Batch [140/217], Batch Loss: 0.2190\n",
            "  Epoch [2/3], Batch [160/217], Batch Loss: 0.2776\n",
            "  Epoch [2/3], Batch [180/217], Batch Loss: 0.5294\n",
            "  Epoch [2/3], Batch [200/217], Batch Loss: 0.5283\n",
            "  Epoch 2/3 took 144.91s\n",
            "    Train Loss: 0.3309, Train Acc: 0.9064\n",
            "    Val Loss:   0.4516, Val Acc:   0.8744\n",
            "  Epoch [3/3], Batch [20/217], Batch Loss: 0.5117\n",
            "  Epoch [3/3], Batch [40/217], Batch Loss: 0.3616\n",
            "  Epoch [3/3], Batch [60/217], Batch Loss: 0.0907\n",
            "  Epoch [3/3], Batch [80/217], Batch Loss: 0.0428\n",
            "  Epoch [3/3], Batch [100/217], Batch Loss: 0.1952\n",
            "  Epoch [3/3], Batch [120/217], Batch Loss: 0.0690\n",
            "  Epoch [3/3], Batch [140/217], Batch Loss: 0.2493\n",
            "  Epoch [3/3], Batch [160/217], Batch Loss: 0.1160\n",
            "  Epoch [3/3], Batch [180/217], Batch Loss: 0.1543\n",
            "  Epoch [3/3], Batch [200/217], Batch Loss: 0.0334\n",
            "  Epoch 3/3 took 157.40s\n",
            "    Train Loss: 0.1890, Train Acc: 0.9417\n",
            "    Val Loss:   0.3745, Val Acc:   0.8963\n",
            "  Training completed for this hyperparameter set.\n",
            "Reported Final Validation Loss for {'epochs': 3, 'lr_fc': 0.001, 'lr_finetune': 0.0001}: 0.3745, Acc: 0.8963\n",
            "==================================================\n",
            "\n",
            "\n",
            "========== Testing Hyperparameters: {'epochs': 3, 'lr_fc': 0.002, 'lr_finetune': 0.0002} ==========\n",
            "Training on cpu for 3 epoch(s)...\n",
            "  Epoch [1/3], Batch [20/217], Batch Loss: 3.1082\n",
            "  Epoch [1/3], Batch [40/217], Batch Loss: 1.6373\n",
            "  Epoch [1/3], Batch [60/217], Batch Loss: 0.7028\n",
            "  Epoch [1/3], Batch [80/217], Batch Loss: 0.3824\n",
            "  Epoch [1/3], Batch [100/217], Batch Loss: 0.9206\n",
            "  Epoch [1/3], Batch [120/217], Batch Loss: 0.6623\n",
            "  Epoch [1/3], Batch [140/217], Batch Loss: 0.8291\n",
            "  Epoch [1/3], Batch [160/217], Batch Loss: 0.8609\n",
            "  Epoch [1/3], Batch [180/217], Batch Loss: 0.6275\n",
            "  Epoch [1/3], Batch [200/217], Batch Loss: 0.7137\n",
            "  Epoch 1/3 took 150.97s\n",
            "    Train Loss: 1.2704, Train Acc: 0.7106\n",
            "    Val Loss:   0.6274, Val Acc:   0.8422\n",
            "  Epoch [2/3], Batch [20/217], Batch Loss: 0.3118\n",
            "  Epoch [2/3], Batch [40/217], Batch Loss: 0.6879\n",
            "  Epoch [2/3], Batch [60/217], Batch Loss: 0.2187\n",
            "  Epoch [2/3], Batch [80/217], Batch Loss: 0.5908\n",
            "  Epoch [2/3], Batch [100/217], Batch Loss: 0.2607\n",
            "  Epoch [2/3], Batch [120/217], Batch Loss: 0.3455\n",
            "  Epoch [2/3], Batch [140/217], Batch Loss: 0.4148\n",
            "  Epoch [2/3], Batch [160/217], Batch Loss: 0.5134\n",
            "  Epoch [2/3], Batch [180/217], Batch Loss: 0.6258\n",
            "  Epoch [2/3], Batch [200/217], Batch Loss: 0.3824\n",
            "  Epoch 2/3 took 153.84s\n",
            "    Train Loss: 0.3618, Train Acc: 0.8927\n",
            "    Val Loss:   0.6113, Val Acc:   0.8543\n",
            "  Epoch [3/3], Batch [20/217], Batch Loss: 0.1768\n",
            "  Epoch [3/3], Batch [40/217], Batch Loss: 0.2735\n",
            "  Epoch [3/3], Batch [60/217], Batch Loss: 0.1885\n",
            "  Epoch [3/3], Batch [80/217], Batch Loss: 0.1872\n",
            "  Epoch [3/3], Batch [100/217], Batch Loss: 0.2892\n",
            "  Epoch [3/3], Batch [120/217], Batch Loss: 0.2293\n",
            "  Epoch [3/3], Batch [140/217], Batch Loss: 0.1850\n",
            "  Epoch [3/3], Batch [160/217], Batch Loss: 0.4648\n",
            "  Epoch [3/3], Batch [180/217], Batch Loss: 0.1594\n",
            "  Epoch [3/3], Batch [200/217], Batch Loss: 0.4899\n",
            "  Epoch 3/3 took 150.73s\n",
            "    Train Loss: 0.2415, Train Acc: 0.9300\n",
            "    Val Loss:   0.5254, Val Acc:   0.8808\n",
            "  Training completed for this hyperparameter set.\n",
            "Reported Final Validation Loss for {'epochs': 3, 'lr_fc': 0.002, 'lr_finetune': 0.0002}: 0.5254, Acc: 0.8808\n",
            "==================================================\n",
            "\n",
            "\n",
            "========== Testing Hyperparameters: {'epochs': 5, 'lr_fc': 0.001, 'lr_finetune': 0.0001} ==========\n",
            "Training on cpu for 5 epoch(s)...\n",
            "  Epoch [1/5], Batch [20/217], Batch Loss: 3.5572\n",
            "  Epoch [1/5], Batch [40/217], Batch Loss: 2.7050\n",
            "  Epoch [1/5], Batch [60/217], Batch Loss: 1.5388\n",
            "  Epoch [1/5], Batch [80/217], Batch Loss: 0.9082\n",
            "  Epoch [1/5], Batch [100/217], Batch Loss: 1.2798\n",
            "  Epoch [1/5], Batch [120/217], Batch Loss: 0.9748\n",
            "  Epoch [1/5], Batch [140/217], Batch Loss: 0.6107\n",
            "  Epoch [1/5], Batch [160/217], Batch Loss: 0.8282\n",
            "  Epoch [1/5], Batch [180/217], Batch Loss: 0.9531\n",
            "  Epoch [1/5], Batch [200/217], Batch Loss: 0.6231\n",
            "  Epoch 1/5 took 149.76s\n",
            "    Train Loss: 1.4675, Train Acc: 0.6787\n",
            "    Val Loss:   0.5747, Val Acc:   0.8450\n",
            "  Epoch [2/5], Batch [20/217], Batch Loss: 0.5255\n",
            "  Epoch [2/5], Batch [40/217], Batch Loss: 0.2432\n",
            "  Epoch [2/5], Batch [60/217], Batch Loss: 0.5782\n",
            "  Epoch [2/5], Batch [80/217], Batch Loss: 0.2076\n",
            "  Epoch [2/5], Batch [100/217], Batch Loss: 0.4833\n",
            "  Epoch [2/5], Batch [120/217], Batch Loss: 0.3198\n",
            "  Epoch [2/5], Batch [140/217], Batch Loss: 0.5305\n",
            "  Epoch [2/5], Batch [160/217], Batch Loss: 0.2928\n",
            "  Epoch [2/5], Batch [180/217], Batch Loss: 0.1796\n",
            "  Epoch [2/5], Batch [200/217], Batch Loss: 0.4268\n",
            "  Epoch 2/5 took 148.88s\n",
            "    Train Loss: 0.3374, Train Acc: 0.9033\n",
            "    Val Loss:   0.4623, Val Acc:   0.8738\n",
            "  Epoch [3/5], Batch [20/217], Batch Loss: 0.2076\n",
            "  Epoch [3/5], Batch [40/217], Batch Loss: 0.1756\n",
            "  Epoch [3/5], Batch [60/217], Batch Loss: 0.1570\n",
            "  Epoch [3/5], Batch [80/217], Batch Loss: 0.2398\n",
            "  Epoch [3/5], Batch [100/217], Batch Loss: 0.0856\n",
            "  Epoch [3/5], Batch [120/217], Batch Loss: 0.2168\n",
            "  Epoch [3/5], Batch [140/217], Batch Loss: 0.2404\n",
            "  Epoch [3/5], Batch [160/217], Batch Loss: 0.0856\n",
            "  Epoch [3/5], Batch [180/217], Batch Loss: 0.0397\n",
            "  Epoch [3/5], Batch [200/217], Batch Loss: 0.3518\n",
            "  Epoch 3/5 took 149.04s\n",
            "    Train Loss: 0.1662, Train Acc: 0.9520\n",
            "    Val Loss:   0.4139, Val Acc:   0.8900\n",
            "  Epoch [4/5], Batch [20/217], Batch Loss: 0.1006\n",
            "  Epoch [4/5], Batch [40/217], Batch Loss: 0.1680\n",
            "  Epoch [4/5], Batch [60/217], Batch Loss: 0.0636\n",
            "  Epoch [4/5], Batch [80/217], Batch Loss: 0.0945\n",
            "  Epoch [4/5], Batch [100/217], Batch Loss: 0.3353\n",
            "  Epoch [4/5], Batch [120/217], Batch Loss: 0.0535\n",
            "  Epoch [4/5], Batch [140/217], Batch Loss: 0.1111\n",
            "  Epoch [4/5], Batch [160/217], Batch Loss: 0.0267\n",
            "  Epoch [4/5], Batch [180/217], Batch Loss: 0.0246\n",
            "  Epoch [4/5], Batch [200/217], Batch Loss: 0.0220\n",
            "  Epoch 4/5 took 128.08s\n",
            "    Train Loss: 0.1154, Train Acc: 0.9660\n",
            "    Val Loss:   0.4116, Val Acc:   0.8952\n",
            "  Epoch [5/5], Batch [20/217], Batch Loss: 0.1091\n",
            "  Epoch [5/5], Batch [40/217], Batch Loss: 0.0195\n",
            "  Epoch [5/5], Batch [60/217], Batch Loss: 0.1720\n",
            "  Epoch [5/5], Batch [80/217], Batch Loss: 0.0882\n",
            "  Epoch [5/5], Batch [100/217], Batch Loss: 0.1629\n",
            "  Epoch [5/5], Batch [120/217], Batch Loss: 0.0129\n",
            "  Epoch [5/5], Batch [140/217], Batch Loss: 0.0220\n",
            "  Epoch [5/5], Batch [160/217], Batch Loss: 0.0427\n",
            "  Epoch [5/5], Batch [180/217], Batch Loss: 0.0446\n",
            "  Epoch [5/5], Batch [200/217], Batch Loss: 0.0572\n",
            "  Epoch 5/5 took 120.75s\n",
            "    Train Loss: 0.0795, Train Acc: 0.9790\n",
            "    Val Loss:   0.3894, Val Acc:   0.9015\n",
            "  Training completed for this hyperparameter set.\n",
            "Reported Final Validation Loss for {'epochs': 5, 'lr_fc': 0.001, 'lr_finetune': 0.0001}: 0.3894, Acc: 0.9015\n",
            "==================================================\n",
            "\n",
            "\n",
            "========== Testing Hyperparameters: {'epochs': 5, 'lr_fc': 0.002, 'lr_finetune': 0.0002} ==========\n",
            "Training on cpu for 5 epoch(s)...\n",
            "  Epoch [1/5], Batch [20/217], Batch Loss: 3.5929\n",
            "  Epoch [1/5], Batch [40/217], Batch Loss: 1.6375\n",
            "  Epoch [1/5], Batch [60/217], Batch Loss: 1.2107\n",
            "  Epoch [1/5], Batch [80/217], Batch Loss: 1.2403\n",
            "  Epoch [1/5], Batch [100/217], Batch Loss: 1.0406\n",
            "  Epoch [1/5], Batch [120/217], Batch Loss: 0.8701\n",
            "  Epoch [1/5], Batch [140/217], Batch Loss: 0.7800\n",
            "  Epoch [1/5], Batch [160/217], Batch Loss: 0.8723\n",
            "  Epoch [1/5], Batch [180/217], Batch Loss: 0.4092\n",
            "  Epoch [1/5], Batch [200/217], Batch Loss: 0.6668\n",
            "  Epoch 1/5 took 122.24s\n",
            "    Train Loss: 1.2875, Train Acc: 0.7065\n",
            "    Val Loss:   0.5300, Val Acc:   0.8646\n",
            "  Epoch [2/5], Batch [20/217], Batch Loss: 0.2905\n",
            "  Epoch [2/5], Batch [40/217], Batch Loss: 0.3842\n",
            "  Epoch [2/5], Batch [60/217], Batch Loss: 0.3795\n",
            "  Epoch [2/5], Batch [80/217], Batch Loss: 0.3291\n",
            "  Epoch [2/5], Batch [100/217], Batch Loss: 0.5770\n",
            "  Epoch [2/5], Batch [120/217], Batch Loss: 0.1816\n",
            "  Epoch [2/5], Batch [140/217], Batch Loss: 0.2478\n",
            "  Epoch [2/5], Batch [160/217], Batch Loss: 0.5562\n",
            "  Epoch [2/5], Batch [180/217], Batch Loss: 0.3251\n",
            "  Epoch [2/5], Batch [200/217], Batch Loss: 0.3842\n",
            "  Epoch 2/5 took 131.59s\n",
            "    Train Loss: 0.3664, Train Acc: 0.8941\n",
            "    Val Loss:   0.5009, Val Acc:   0.8669\n",
            "  Epoch [3/5], Batch [20/217], Batch Loss: 0.2805\n",
            "  Epoch [3/5], Batch [40/217], Batch Loss: 0.2402\n",
            "  Epoch [3/5], Batch [60/217], Batch Loss: 0.1792\n",
            "  Epoch [3/5], Batch [80/217], Batch Loss: 0.0738\n",
            "  Epoch [3/5], Batch [100/217], Batch Loss: 0.1842\n",
            "  Epoch [3/5], Batch [120/217], Batch Loss: 0.2597\n",
            "  Epoch [3/5], Batch [140/217], Batch Loss: 0.4558\n",
            "  Epoch [3/5], Batch [160/217], Batch Loss: 0.4002\n",
            "  Epoch [3/5], Batch [180/217], Batch Loss: 0.0849\n",
            "  Epoch [3/5], Batch [200/217], Batch Loss: 0.1380\n",
            "  Epoch 3/5 took 126.19s\n",
            "    Train Loss: 0.2098, Train Acc: 0.9375\n",
            "    Val Loss:   0.5127, Val Acc:   0.8738\n",
            "  Epoch [4/5], Batch [20/217], Batch Loss: 0.1901\n",
            "  Epoch [4/5], Batch [40/217], Batch Loss: 0.0421\n",
            "  Epoch [4/5], Batch [60/217], Batch Loss: 0.0290\n",
            "  Epoch [4/5], Batch [80/217], Batch Loss: 0.2679\n",
            "  Epoch [4/5], Batch [100/217], Batch Loss: 0.1464\n",
            "  Epoch [4/5], Batch [120/217], Batch Loss: 0.1827\n",
            "  Epoch [4/5], Batch [140/217], Batch Loss: 0.1181\n",
            "  Epoch [4/5], Batch [160/217], Batch Loss: 0.0900\n",
            "  Epoch [4/5], Batch [180/217], Batch Loss: 0.2352\n",
            "  Epoch [4/5], Batch [200/217], Batch Loss: 0.3823\n",
            "  Epoch 4/5 took 147.06s\n",
            "    Train Loss: 0.1695, Train Acc: 0.9455\n",
            "    Val Loss:   0.4830, Val Acc:   0.8888\n",
            "  Epoch [5/5], Batch [20/217], Batch Loss: 0.1885\n",
            "  Epoch [5/5], Batch [40/217], Batch Loss: 0.1943\n",
            "  Epoch [5/5], Batch [60/217], Batch Loss: 0.0202\n",
            "  Epoch [5/5], Batch [80/217], Batch Loss: 0.0629\n",
            "  Epoch [5/5], Batch [100/217], Batch Loss: 0.0752\n",
            "  Epoch [5/5], Batch [120/217], Batch Loss: 0.1119\n",
            "  Epoch [5/5], Batch [140/217], Batch Loss: 0.2461\n",
            "  Epoch [5/5], Batch [160/217], Batch Loss: 0.0922\n",
            "  Epoch [5/5], Batch [180/217], Batch Loss: 0.1541\n",
            "  Epoch [5/5], Batch [200/217], Batch Loss: 0.2682\n",
            "  Epoch 5/5 took 156.09s\n",
            "    Train Loss: 0.1463, Train Acc: 0.9550\n",
            "    Val Loss:   0.5136, Val Acc:   0.8888\n",
            "  Training completed for this hyperparameter set.\n",
            "Reported Final Validation Loss for {'epochs': 5, 'lr_fc': 0.002, 'lr_finetune': 0.0002}: 0.5136, Acc: 0.8888\n",
            "==================================================\n",
            "\n",
            "\n",
            "\n",
            "==================== Hyperparameter Search Full Report ====================\n",
            "Top performing hyperparameter sets (by validation loss):\n",
            "  Params: Epochs=3, FC LR=0.001, FT LR=0.0001 -> Final Val Loss: 0.3745, Final Val Acc: 0.8963\n",
            "  Params: Epochs=5, FC LR=0.001, FT LR=0.0001 -> Final Val Loss: 0.3894, Final Val Acc: 0.9015\n",
            "  Params: Epochs=5, FC LR=0.002, FT LR=0.0002 -> Final Val Loss: 0.5136, Final Val Acc: 0.8888\n",
            "  Params: Epochs=3, FC LR=0.002, FT LR=0.0002 -> Final Val Loss: 0.5254, Final Val Acc: 0.8808\n",
            "  Params: Epochs=1, FC LR=0.001, FT LR=0.0001 -> Final Val Loss: 0.5565, Final Val Acc: 0.8485\n",
            "  Params: Epochs=1, FC LR=0.002, FT LR=0.0002 -> Final Val Loss: 0.6154, Final Val Acc: 0.8306\n",
            "\n",
            "--- Hyperparameter Search Cell Execution Finished (No TensorBoard Logging) ---\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "import os\n",
        "# Removed: from torch.utils.tensorboard import SummaryWriter\n",
        "import torch\n",
        "import torch.nn as nn # In case criterion was not made global\n",
        "\n",
        "print(\"--- Starting Hyperparameter Search (No TensorBoard Logging) ---\")\n",
        "\n",
        "# --- 0. Prerequisite Check & Setup ---\n",
        "# Ensure essential variables from previous cells are available.\n",
        "required_globals = ['device', 'train_loader', 'val_loader', 'current_num_classes', \n",
        "                    'get_model', 'criterion', 'DATA_DIR', 'IMAGE_SIZE', 'BATCH_SIZE',\n",
        "                    'TRAIN_SPLIT_RATIO', 'RANDOM_SEED']\n",
        "missing_vars = [var for var in required_globals if var not in globals()]\n",
        "\n",
        "if missing_vars:\n",
        "    print(f\"Error: Essential variables not found in global scope: {missing_vars}\")\n",
        "    print(\"Please ensure the main training setup cells (defining imports, configs, data loaders, model function, etc.) have been run.\")\n",
        "    # Depending on your notebook structure, you might want to stop execution here\n",
        "    # For example: raise RuntimeError(\"Missing prerequisite variables for hyperparameter search.\")\n",
        "else:\n",
        "    print(\"Prerequisite variables found. Proceeding with hyperparameter search.\")\n",
        "\n",
        "    # --- 1. Define Hyperparameter Space ---\n",
        "    num_epochs_options = [1, 3, 5]\n",
        "    lr_fc_options = [0.001, 0.002]\n",
        "    #lr_finetune_options = [0.0001, 0.0002]\n",
        "\n",
        "    hyperparameter_search_results = []\n",
        "\n",
        "    # --- 2. Define a modified train_model function for this cell that returns final validation loss ---\n",
        "    # (TensorBoard 'writer' parameter and related calls are removed)\n",
        "    def train_model_for_hp_search_no_tb(model, train_loader, val_loader, criterion_fn, optimizer, num_epochs, device):\n",
        "        print(f\"Training on {device} for {num_epochs} epoch(s)...\")\n",
        "        log_indent = \"  \" \n",
        "        \n",
        "        final_epoch_val_loss = float('inf')\n",
        "        final_epoch_val_acc = 0.0\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            model.train()\n",
        "            current_epoch_train_loss = 0.0\n",
        "            current_epoch_correct_train = 0\n",
        "            current_epoch_total_train = 0\n",
        "\n",
        "            for i, (inputs, labels) in enumerate(train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_fn(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                current_epoch_train_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                current_epoch_total_train += labels.size(0)\n",
        "                current_epoch_correct_train += (predicted == labels).sum().item()\n",
        "                \n",
        "                if (i + 1) % 20 == 0: \n",
        "                    print(f'{log_indent}Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}')\n",
        "\n",
        "            avg_epoch_train_loss = current_epoch_train_loss / current_epoch_total_train if current_epoch_total_train > 0 else 0\n",
        "            avg_epoch_train_acc = current_epoch_correct_train / current_epoch_total_train if current_epoch_total_train > 0 else 0\n",
        "            \n",
        "            # Validation phase\n",
        "            model.eval()\n",
        "            current_epoch_val_loss = 0.0\n",
        "            current_epoch_correct_val = 0\n",
        "            current_epoch_total_val = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion_fn(outputs, labels)\n",
        "                    current_epoch_val_loss += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    current_epoch_total_val += labels.size(0)\n",
        "                    current_epoch_correct_val += (predicted == labels).sum().item()\n",
        "            \n",
        "            avg_epoch_val_loss = current_epoch_val_loss / current_epoch_total_val if current_epoch_total_val > 0 else float('inf')\n",
        "            avg_epoch_val_acc = current_epoch_correct_val / current_epoch_total_val if current_epoch_total_val > 0 else 0.0\n",
        "            \n",
        "            if epoch == num_epochs - 1:\n",
        "                final_epoch_val_loss = avg_epoch_val_loss\n",
        "                final_epoch_val_acc = avg_epoch_val_acc\n",
        "            \n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f'{log_indent}Epoch {epoch+1}/{num_epochs} took {epoch_duration:.2f}s')\n",
        "            print(f'{log_indent}  Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {avg_epoch_train_acc:.4f}')\n",
        "            print(f'{log_indent}  Val Loss:   {avg_epoch_val_loss:.4f}, Val Acc:   {avg_epoch_val_acc:.4f}')\n",
        "\n",
        "        print(f\"{log_indent}Training completed for this hyperparameter set.\")\n",
        "        return model, final_epoch_val_loss, final_epoch_val_acc\n",
        "    # --- End of modified train_model_for_hp_search_no_tb ---\n",
        "\n",
        "    # --- 3. Iterate through Hyperparameters ---\n",
        "    if not missing_vars: \n",
        "        for n_epochs_hp in num_epochs_options:\n",
        "            for fc_lr_hp in lr_fc_options:\n",
        "                ft_lr_hp = fc_lr_hp * 0.1               \n",
        "                current_hparams_values = {\n",
        "                    \"epochs\": n_epochs_hp,\n",
        "                    \"lr_fc\": fc_lr_hp,\n",
        "                    \"lr_finetune\": ft_lr_hp\n",
        "                }\n",
        "                print(f\"\\n{'='*10} Testing Hyperparameters: {current_hparams_values} {'='*10}\")\n",
        "\n",
        "                model_hp, optimizer_hp = get_model(\n",
        "                    num_classes=current_num_classes, \n",
        "                    learning_rate_fc=fc_lr_hp,\n",
        "                    learning_rate_finetune=ft_lr_hp\n",
        "                )\n",
        "                model_hp.to(device)\n",
        "\n",
        "                # Train model using the modified function (no writer)\n",
        "                _, final_val_loss_run, final_val_acc_run = train_model_for_hp_search_no_tb(\n",
        "                    model_hp,\n",
        "                    train_loader, \n",
        "                    val_loader,   \n",
        "                    criterion,    \n",
        "                    optimizer_hp,\n",
        "                    n_epochs_hp,\n",
        "                    device\n",
        "                )\n",
        "                \n",
        "                hyperparameter_search_results.append({\n",
        "                    \"params\": current_hparams_values,\n",
        "                    \"final_val_loss\": final_val_loss_run,\n",
        "                    \"final_val_acc\": final_val_acc_run\n",
        "                })\n",
        "                print(f\"Reported Final Validation Loss for {current_hparams_values}: {final_val_loss_run:.4f}, Acc: {final_val_acc_run:.4f}\")\n",
        "                print(f\"{'='*50}\\n\")\n",
        "\n",
        "        # --- 4. Print Summary of Hyperparameter Search ----\n",
        "        print(f\"\\n\\n{'='*20} Hyperparameter Search Full Report {'='*20}\")\n",
        "        if hyperparameter_search_results:\n",
        "            sorted_results = sorted(hyperparameter_search_results, key=lambda x: x['final_val_loss'])\n",
        "            print(\"Top performing hyperparameter sets (by validation loss):\")\n",
        "            for result in sorted_results:\n",
        "                print(f\"  Params: Epochs={result['params']['epochs']}, FC LR={result['params']['lr_fc']}, FT LR={result['params']['lr_finetune']} \"\n",
        "                      f\"-> Final Val Loss: {result['final_val_loss']:.4f}, Final Val Acc: {result['final_val_acc']:.4f}\")\n",
        "        else:\n",
        "            print(\"No hyperparameter search results were collected (or an error occurred).\")\n",
        "\n",
        "print(\"\\n--- Hyperparameter Search Cell Execution Finished (No TensorBoard Logging) ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b70c8a2f",
      "metadata": {},
      "source": [
        "就如我们所选择的，最好的超参数是训练 5 个 epoch，使用普通学习率 0.001，微调学习率 0.001。训练集上分类准确率达到了 90.15%。显而易见的是，继续训练，准确率会继续上升。可惜我的计算资源过于有限。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "05b9bc9c",
      "metadata": {},
      "source": [
        "# 6 与从头训练的模型做比较\n",
        "\n",
        "我们从头训练一个 resnet-18 模型。我们训练它 5 个 epoch。注意由于所有参数都要被训练，它已经使用了远超过我们模型的计算资源了。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "id": "4d1f1b99",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Training ResNet-18 From Scratch ---\n",
            "Prerequisite variables found. Proceeding with training from scratch.\n",
            "\n",
            "Instantiating ResNet-18 for training from scratch with 101 classes.\n",
            "Training from scratch on cpu for 5 epoch(s)...\n",
            "  Epoch [1/5], Batch [20/217], Batch Loss: 4.1954\n",
            "  Epoch [1/5], Batch [40/217], Batch Loss: 3.5331\n",
            "  Epoch [1/5], Batch [60/217], Batch Loss: 3.6803\n",
            "  Epoch [1/5], Batch [80/217], Batch Loss: 3.8752\n",
            "  Epoch [1/5], Batch [100/217], Batch Loss: 2.8735\n",
            "  Epoch [1/5], Batch [120/217], Batch Loss: 3.1450\n",
            "  Epoch [1/5], Batch [140/217], Batch Loss: 2.9949\n",
            "  Epoch [1/5], Batch [160/217], Batch Loss: 3.0378\n",
            "  Epoch [1/5], Batch [180/217], Batch Loss: 2.5939\n",
            "  Epoch [1/5], Batch [200/217], Batch Loss: 2.5988\n",
            "  Epoch 1/5 took 202.34s\n",
            "    Train Loss: 3.3635, Train Acc: 0.3062\n",
            "    Val Loss:   2.9619, Val Acc:   0.3669\n",
            "  Epoch [2/5], Batch [20/217], Batch Loss: 2.9442\n",
            "  Epoch [2/5], Batch [40/217], Batch Loss: 2.2189\n",
            "  Epoch [2/5], Batch [60/217], Batch Loss: 2.1546\n",
            "  Epoch [2/5], Batch [80/217], Batch Loss: 2.6541\n",
            "  Epoch [2/5], Batch [100/217], Batch Loss: 2.6158\n",
            "  Epoch [2/5], Batch [120/217], Batch Loss: 2.3006\n",
            "  Epoch [2/5], Batch [140/217], Batch Loss: 1.6309\n",
            "  Epoch [2/5], Batch [160/217], Batch Loss: 2.7289\n",
            "  Epoch [2/5], Batch [180/217], Batch Loss: 2.0090\n",
            "  Epoch [2/5], Batch [200/217], Batch Loss: 2.3341\n",
            "  Epoch 2/5 took 210.41s\n",
            "    Train Loss: 2.4615, Train Acc: 0.4360\n",
            "    Val Loss:   2.3176, Val Acc:   0.4758\n",
            "  Epoch [3/5], Batch [20/217], Batch Loss: 1.9801\n",
            "  Epoch [3/5], Batch [40/217], Batch Loss: 1.8921\n",
            "  Epoch [3/5], Batch [60/217], Batch Loss: 2.2631\n",
            "  Epoch [3/5], Batch [80/217], Batch Loss: 1.7503\n",
            "  Epoch [3/5], Batch [100/217], Batch Loss: 2.2692\n",
            "  Epoch [3/5], Batch [120/217], Batch Loss: 2.5458\n",
            "  Epoch [3/5], Batch [140/217], Batch Loss: 1.7628\n",
            "  Epoch [3/5], Batch [160/217], Batch Loss: 1.9600\n",
            "  Epoch [3/5], Batch [180/217], Batch Loss: 1.4867\n",
            "  Epoch [3/5], Batch [200/217], Batch Loss: 1.8945\n",
            "  Epoch 3/5 took 182.67s\n",
            "    Train Loss: 1.9140, Train Acc: 0.5380\n",
            "    Val Loss:   1.9686, Val Acc:   0.5415\n",
            "  Epoch [4/5], Batch [20/217], Batch Loss: 1.7182\n",
            "  Epoch [4/5], Batch [40/217], Batch Loss: 0.9067\n",
            "  Epoch [4/5], Batch [60/217], Batch Loss: 1.2823\n",
            "  Epoch [4/5], Batch [80/217], Batch Loss: 0.5985\n",
            "  Epoch [4/5], Batch [100/217], Batch Loss: 1.8235\n",
            "  Epoch [4/5], Batch [120/217], Batch Loss: 1.6209\n",
            "  Epoch [4/5], Batch [140/217], Batch Loss: 1.8849\n",
            "  Epoch [4/5], Batch [160/217], Batch Loss: 1.9913\n",
            "  Epoch [4/5], Batch [180/217], Batch Loss: 1.3746\n",
            "  Epoch [4/5], Batch [200/217], Batch Loss: 1.8008\n",
            "  Epoch 4/5 took 208.03s\n",
            "    Train Loss: 1.5926, Train Acc: 0.6014\n",
            "    Val Loss:   1.8230, Val Acc:   0.5680\n",
            "  Epoch [5/5], Batch [20/217], Batch Loss: 1.5455\n",
            "  Epoch [5/5], Batch [40/217], Batch Loss: 1.2969\n",
            "  Epoch [5/5], Batch [60/217], Batch Loss: 1.4295\n",
            "  Epoch [5/5], Batch [80/217], Batch Loss: 0.8753\n",
            "  Epoch [5/5], Batch [100/217], Batch Loss: 1.1595\n",
            "  Epoch [5/5], Batch [120/217], Batch Loss: 1.5137\n",
            "  Epoch [5/5], Batch [140/217], Batch Loss: 2.2205\n",
            "  Epoch [5/5], Batch [160/217], Batch Loss: 1.8836\n",
            "  Epoch [5/5], Batch [180/217], Batch Loss: 1.6758\n",
            "  Epoch [5/5], Batch [200/217], Batch Loss: 1.5191\n",
            "  Epoch 5/5 took 219.57s\n",
            "    Train Loss: 1.3479, Train Acc: 0.6509\n",
            "    Val Loss:   1.4986, Val Acc:   0.6262\n",
            "  Training from scratch completed.\n",
            "\n",
            "Total time for scratch training (5 epochs): 1023.01 seconds.\n",
            "\n",
            "Summary of Training from Scratch:\n",
            "  Epoch 1: Train Loss=3.3635, Train Acc=0.3062, Val Loss=2.9619, Val Acc=0.3669\n",
            "  Epoch 2: Train Loss=2.4615, Train Acc=0.4360, Val Loss=2.3176, Val Acc=0.4758\n",
            "  Epoch 3: Train Loss=1.9140, Train Acc=0.5380, Val Loss=1.9686, Val Acc=0.5415\n",
            "  Epoch 4: Train Loss=1.5926, Train Acc=0.6014, Val Loss=1.8230, Val Acc=0.5680\n",
            "  Epoch 5: Train Loss=1.3479, Train Acc=0.6509, Val Loss=1.4986, Val Acc=0.6262\n",
            "\n",
            "--- Scratch Training Cell Execution Finished ---\n"
          ]
        }
      ],
      "source": [
        "print(\"--- Training ResNet-18 From Scratch ---\")\n",
        "\n",
        "# --- 0. Configuration for Scratch Training ---\n",
        "# Most configurations like DATA_DIR, IMAGE_SIZE, BATCH_SIZE, etc., are assumed to be globally defined.\n",
        "# RANDOM_SEED is also assumed global for consistent data splitting.\n",
        "SCRATCH_NUM_EPOCHS = 5\n",
        "SCRATCH_LEARNING_RATE = 0.001 # A single learning rate for all parameters\n",
        "\n",
        "# --- 1. Prerequisite Check ---\n",
        "required_globals_scratch = ['device', 'train_loader', 'val_loader', 'current_num_classes', \n",
        "                            'criterion', 'IMAGE_SIZE', 'BATCH_SIZE', 'RANDOM_SEED']\n",
        "missing_vars_scratch = [var for var in required_globals_scratch if var not in globals()]\n",
        "\n",
        "if missing_vars_scratch:\n",
        "    print(f\"Error: Essential variables not found in global scope: {missing_vars_scratch}\")\n",
        "    print(\"Please ensure the main training setup cells (defining imports, configs, data loaders, etc.) have been run.\")\n",
        "else:\n",
        "    print(\"Prerequisite variables found. Proceeding with training from scratch.\")\n",
        "\n",
        "    # --- 2. Define Model Instantiation for Scratch Training ---\n",
        "    def get_resnet18_scratch(num_classes):\n",
        "        # Initialize ResNet-18 WITHOUT pre-trained weights\n",
        "        model_scratch = torchvision.models.resnet18(weights=None) # Key change: weights=None\n",
        "        \n",
        "        # Modify the final fully connected layer for the number of classes\n",
        "        num_ftrs_scratch = model_scratch.fc.in_features\n",
        "        model_scratch.fc = nn.Linear(num_ftrs_scratch, num_classes)\n",
        "        \n",
        "        # Initialize all parameters using Xavier initialization\n",
        "        for m in model_scratch.modules():\n",
        "            if isinstance(m, (nn.Conv2d, nn.Linear)):\n",
        "                nn.init.xavier_uniform_(m.weight)\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "        # The following lines specifically initialize the fc layer, which is redundant\n",
        "        # if the loop above correctly handles all Linear layers.\n",
        "        # However, keeping it for explicitness or if there's a specific reason\n",
        "        # to re-initialize it (though generally not needed).\n",
        "        # If the loop above is comprehensive, these two lines can be removed.\n",
        "\n",
        "       # nn.init.xavier_uniform_(model_scratch.fc.weight)\n",
        "        # if model_scratch.fc.bias is not None:\n",
        "        #     nn.init.zeros_(model_scratch.fc.bias)\n",
        "            \n",
        "        # For training from scratch, all parameters should have requires_grad = True by default\n",
        "        # So, no need to manually set requires_grad as we did for fine-tuning.\n",
        "        # All parameters will be passed to the optimizer.\n",
        "        return model_scratch\n",
        "\n",
        "    # --- 3. Define Training Function (similar to previous, no TensorBoard) ---\n",
        "    def train_model_scratch_epochs(model, train_loader, val_loader, criterion_fn, optimizer, num_epochs, device):\n",
        "        print(f\"Training from scratch on {device} for {num_epochs} epoch(s)...\")\n",
        "        log_indent = \"  \"\n",
        "        \n",
        "        epoch_summary = []\n",
        "\n",
        "        for epoch in range(num_epochs):\n",
        "            epoch_start_time = time.time()\n",
        "            model.train()\n",
        "            current_epoch_train_loss_sum = 0.0\n",
        "            current_epoch_correct_train = 0\n",
        "            current_epoch_total_train = 0\n",
        "\n",
        "            for i, (inputs, labels) in enumerate(train_loader):\n",
        "                inputs, labels = inputs.to(device), labels.to(device)\n",
        "                optimizer.zero_grad()\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion_fn(outputs, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                current_epoch_train_loss_sum += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                current_epoch_total_train += labels.size(0)\n",
        "                current_epoch_correct_train += (predicted == labels).sum().item()\n",
        "                \n",
        "                if (i + 1) % 20 == 0: \n",
        "                    print(f'{log_indent}Epoch [{epoch+1}/{num_epochs}], Batch [{i+1}/{len(train_loader)}], Batch Loss: {loss.item():.4f}')\n",
        "\n",
        "            avg_epoch_train_loss = current_epoch_train_loss_sum / current_epoch_total_train if current_epoch_total_train > 0 else 0\n",
        "            avg_epoch_train_acc = current_epoch_correct_train / current_epoch_total_train if current_epoch_total_train > 0 else 0\n",
        "            \n",
        "            model.eval()\n",
        "            current_epoch_val_loss_sum = 0.0\n",
        "            current_epoch_correct_val = 0\n",
        "            current_epoch_total_val = 0\n",
        "            with torch.no_grad():\n",
        "                for inputs, labels in val_loader:\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion_fn(outputs, labels)\n",
        "                    current_epoch_val_loss_sum += loss.item() * inputs.size(0)\n",
        "                    _, predicted = torch.max(outputs.data, 1)\n",
        "                    current_epoch_total_val += labels.size(0)\n",
        "                    current_epoch_correct_val += (predicted == labels).sum().item()\n",
        "            \n",
        "            avg_epoch_val_loss = current_epoch_val_loss_sum / current_epoch_total_val if current_epoch_total_val > 0 else float('inf')\n",
        "            avg_epoch_val_acc = current_epoch_correct_val / current_epoch_total_val if current_epoch_total_val > 0 else 0.0\n",
        "            \n",
        "            epoch_duration = time.time() - epoch_start_time\n",
        "            print(f'{log_indent}Epoch {epoch+1}/{num_epochs} took {epoch_duration:.2f}s')\n",
        "            print(f'{log_indent}  Train Loss: {avg_epoch_train_loss:.4f}, Train Acc: {avg_epoch_train_acc:.4f}')\n",
        "            print(f'{log_indent}  Val Loss:   {avg_epoch_val_loss:.4f}, Val Acc:   {avg_epoch_val_acc:.4f}')\n",
        "            epoch_summary.append({\n",
        "                'epoch': epoch + 1,\n",
        "                'train_loss': avg_epoch_train_loss,\n",
        "                'train_acc': avg_epoch_train_acc,\n",
        "                'val_loss': avg_epoch_val_loss,\n",
        "                'val_acc': avg_epoch_val_acc\n",
        "            })\n",
        "\n",
        "        print(f\"{log_indent}Training from scratch completed.\")\n",
        "        return model, epoch_summary\n",
        "    # --- End of train_model_scratch_epochs ---\n",
        "\n",
        "    # --- 4. Execute Scratch Training ---\n",
        "    if not missing_vars_scratch:\n",
        "        print(f\"\\nInstantiating ResNet-18 for training from scratch with {current_num_classes} classes.\")\n",
        "        model_scratch_instance = get_resnet18_scratch(num_classes=current_num_classes)\n",
        "        model_scratch_instance.to(device)\n",
        "\n",
        "        # Optimizer for scratch training - all parameters with the same learning rate\n",
        "        optimizer_scratch = optim.Adam(model_scratch_instance.parameters(), lr=SCRATCH_LEARNING_RATE)\n",
        "        \n",
        "        # 'criterion' should be globally defined (e.g., criterion = nn.CrossEntropyLoss())\n",
        "        scratch_start_time = time.time()\n",
        "        \n",
        "        trained_model_scratch, scratch_training_summary = train_model_scratch_epochs(\n",
        "            model_scratch_instance,\n",
        "            train_loader, # Global train_loader\n",
        "            val_loader,   # Global val_loader\n",
        "            criterion,    # Global criterion\n",
        "            optimizer_scratch,\n",
        "            SCRATCH_NUM_EPOCHS,\n",
        "            device        # Global device\n",
        "        )\n",
        "        \n",
        "        scratch_total_time = time.time() - scratch_start_time\n",
        "        print(f\"\\nTotal time for scratch training ({SCRATCH_NUM_EPOCHS} epochs): {scratch_total_time:.2f} seconds.\")\n",
        "\n",
        "        print(\"\\nSummary of Training from Scratch:\")\n",
        "        for item in scratch_training_summary:\n",
        "            print(f\"  Epoch {item['epoch']}: Train Loss={item['train_loss']:.4f}, Train Acc={item['train_acc']:.4f}, \"\n",
        "                  f\"Val Loss={item['val_loss']:.4f}, Val Acc={item['val_acc']:.4f}\")\n",
        "        \n",
        "        # Optionally, save this model's parameters too\n",
        "        # scratch_model_save_path = \"model_parameters_scratch.pth\"\n",
        "        # torch.save(trained_model_scratch.state_dict(), scratch_model_save_path)\n",
        "        # print(f\"Scratch model parameters saved to: {os.path.abspath(scratch_model_save_path)}\")\n",
        "\n",
        "print(\"\\n--- Scratch Training Cell Execution Finished ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6b85be57",
      "metadata": {},
      "source": [
        "显而易见，从头训练的accuracy比微调差了许多。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "33b161b2",
      "metadata": {},
      "source": [
        "# 7 github 仓库地址及权重文件地址\n",
        "\n",
        "github 仓库：见 https://github.com/RuiyuanHuang/NN_and_DL_Mideterm1/tree/main\n",
        "\n",
        "权重文件：见 https://drive.google.com/file/d/1TpXDjjGhcojaFSK53TGxED5ZgdDrY3OV/view?usp=drive_link"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
